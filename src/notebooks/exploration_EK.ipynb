{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "try:\n",
    "    from utils import data_loading\n",
    "except:\n",
    "    sys.path.append('../utils')\n",
    "    sys.path.append('../')\n",
    "    from utils import data_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------\n",
      "Loading subject data with subject ID 1...\n",
      "Current project directory: /Users/emilykruger/Documents/GitHub/aml_project_2023\n",
      "\n",
      "\n",
      "----------------\n",
      "Loading subject data with subject ID 2...\n",
      "Current project directory: /Users/emilykruger/Documents/GitHub/aml_project_2023\n",
      "\n",
      "----------------\n",
      "Initialize CustomDataset\n",
      "\n",
      "Number of samples:  200\n",
      "Transform:  ToTensor()\n",
      "PCA:  PCA(n_components=100)\n",
      "-------\n",
      "Data loaded\n",
      "\n",
      "Data:  200 *  ([<PIL.Image.Image image mode=RGB size=425x425 at 0x7FB53CFD2EE0>, 1], array([-0.8617882 , -0.20318632, -0.62639767, ..., -0.41889378,\n",
      "       -0.60231453, -0.67537224], dtype=float32))\n",
      "Output_concat:  200 * 39548 :  [-0.8617882  -0.20318632 -0.62639767 ... -0.41889378 -0.60231453\n",
      " -0.67537224]\n",
      "\n",
      "Dataset made up of  200 truples? of data\n",
      "--------\n",
      "Shape of 1st element: torch.Size([3, 425, 425])\n",
      "Type of 2nd element: <class 'int'>\n",
      "Shape of 3rd element: torch.Size([100]) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import data_loading, model\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "# Create concatenated lists including X samples * 8 subjects\n",
    "brain_concat = []\n",
    "images_concat = []\n",
    "ids_concat = []\n",
    "\n",
    "for subj in range(1,2+1):\n",
    "    lh, rh, images, id_list  = data_loading.load_subject_data(subj, 0, 100, include_subject_id=True)\n",
    "    brain_concat.extend(np.concatenate((lh, rh), axis=1)) ### investigate whether concat of lh and rh results in what we want\n",
    "    images_concat += images\n",
    "    ids_concat += id_list\n",
    "\n",
    "# Create dataset with concatenated hemispheres\n",
    "dataset = data_loading.CustomDataset(images_list = images_concat, outputs_list = brain_concat, id_list = ids_concat, transform=transforms.ToTensor(), PCA = PCA(n_components = 100))\n",
    "print('\\nDataset made up of ', len(dataset), 'truples? of data\\n--------')\n",
    "print('Shape of 1st element:', dataset[0][0].shape)\n",
    "print('Type of 2nd element:', type(dataset[0][1]))\n",
    "print('Shape of 3rd element:', dataset[0][2].shape, '\\n\\n')\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Create a train and validation subset of the variable dataset with torch\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Custom subset class to preserve .id_list attribute when splitting into train and val\n",
    "class CustomSubset(Subset):\n",
    "    def __init__(self, dataset, indices, id_list):\n",
    "        super(CustomSubset, self).__init__(dataset, indices)\n",
    "        self.id_list = id_list\n",
    "\n",
    "# Use the CustomSubset class for the train and validation subsets\n",
    "train_dataset = CustomSubset(dataset, range(0, train_size), ids_concat[:train_size])\n",
    "val_dataset = CustomSubset(dataset, range(train_size, len(dataset)), ids_concat[train_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 10/10 [02:37<00:00, 15.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 150866.4048553467\n",
      "Validation Loss: 223941457084416.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 10/10 [06:32<00:00, 39.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 15951.304989624023\n",
      "Validation Loss: 3453166416.0\n"
     ]
    }
   ],
   "source": [
    "# Put train dataset into a loader with 2 batches and put test data in val loader\n",
    "train_sampler = data_loading.SubjectSampler(train_dataset)\n",
    "val_sampler = data_loading.SubjectSampler(val_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, sampler=val_sampler)\n",
    "\n",
    "# Initialize model, trainer, optimizer and loss function\n",
    "reg_model = model.ResNet1HeadID(100)\n",
    "trainer = model.Trainer()\n",
    "optimizer = torch.optim.Adam\n",
    "loss = torch.nn.MSELoss()\n",
    "trainer.compile(reg_model, optimizer, learning_rate=0.1, loss_fn=loss)\n",
    "\n",
    "trainer.fitID(num_epochs = 2, train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
