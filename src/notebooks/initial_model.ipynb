{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add utils as a module and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "try:\n",
    "    from utils import data_loading\n",
    "except:\n",
    "    sys.path.append('../utils')\n",
    "    sys.path.append('../')\n",
    "    from utils import data_loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils import model\n",
    "from pathlib import Path\n",
    "from PIL import Image'\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 19004)\n",
      "(200, 20544)\n",
      "(425, 425, 3)\n"
     ]
    }
   ],
   "source": [
    "#loading data for subject 1, first 10 images\n",
    "lh, rh, images  = data_loading.load_subject_data(1, 0, 200)\n",
    "\n",
    "print(lh.shape)\n",
    "print(rh.shape)\n",
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#concat lh and rh\n",
    "brain = np.concatenate((lh, rh), axis=1)\n",
    "brain_separate = [lh, rh]\n",
    "print(len(brain_separate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_loading.CustomDataset(images_list= images, outputs_list= brain, transform=transforms.ToTensor(), PCA = PCA(n_components = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a train and validation subset of variable dataset with torch\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "#put train dataset into a loader with 2 batches and put test data in val loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = model.ResNet2HeadModel(100)\n",
    "trainer = model.Trainer()\n",
    "optimizer = torch.optim.Adam\n",
    "loss = torch.nn.MSELoss()\n",
    "trainer.compile(reg_model, optimizer, learning_rate=0.0001, loss_fn=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aleksygalkowski/Documents/Projects/ucph/social-data-science/2023 sem 3/aml-itu/aml_project_2023/src/notebooks/initial_model.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aleksygalkowski/Documents/Projects/ucph/social-data-science/2023%20sem%203/aml-itu/aml_project_2023/src/notebooks/initial_model.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(num_epochs\u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader)\n",
      "File \u001b[0;32m~/Documents/Projects/ucph/social-data-science/2023 sem 3/aml-itu/aml_project_2023/src/notebooks/../utils/model.py:62\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, num_epochs, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     61\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 62\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader:\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     64\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "trainer.fit(num_epochs= 2, train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aleksygalkowski/Documents/Projects/ucph/social-data-science/2023 sem 3/aml-itu/aml_project_2023/src/notebooks/initial_model.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aleksygalkowski/Documents/Projects/ucph/social-data-science/2023%20sem%203/aml-itu/aml_project_2023/src/notebooks/initial_model.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mhistory\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and training history saved to initial_model.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.save('initial_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer  = model.Trainer()\n",
    "trainer.compile(reg_model, optimizer, learning_rate=0.0001, loss_fn=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and training history loaded from initial_model.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.load('initial_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [170.70301513671876, 167.72671508789062],\n",
       " 'val_loss': [183.5020439147949, 183.56376399993897]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset1(Dataset):\n",
    "    def __init__(self, images_list, outputs_list1, outputs_list2, transform=None, PCA1=None, PCA2=None):\n",
    "        self.num_samples = len(images_list)\n",
    "        self.transform = transform\n",
    "        self.PCA1 = PCA1\n",
    "        self.PCA2 = PCA2\n",
    "        self.data, self.output1, self.output2 = self.load_data(images_list, outputs_list1, outputs_list2)\n",
    "\n",
    "    def load_data(self, images_list, outputs_list1, outputs_list2):\n",
    "        data = []\n",
    "        output_concat1 = []\n",
    "        output_concat2 = []\n",
    "\n",
    "        for i in range(self.num_samples):\n",
    "            # Load image from the given list\n",
    "            image = Image.fromarray(images_list[i])\n",
    "\n",
    "            # Load output arrays from the given lists\n",
    "            output1 = outputs_list1[i]\n",
    "            output2 = outputs_list2[i]\n",
    "\n",
    "            data.append((image, output1, output2))\n",
    "            output_concat1.append(output1)\n",
    "            output_concat2.append(output2)\n",
    "\n",
    "        if self.PCA1:\n",
    "            self.PCA1.fit(output_concat1)\n",
    "\n",
    "        if self.PCA2:\n",
    "            self.PCA2.fit(output_concat2)\n",
    "\n",
    "        return data, output_concat1, output_concat2\n",
    "\n",
    "    def give_output(self):\n",
    "        return self.output1, self.output2\n",
    "\n",
    "    def get_PCA1(self):\n",
    "        return self.PCA1\n",
    "\n",
    "    def get_PCA2(self):\n",
    "        return self.PCA2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, output1, output2 = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.PCA1:\n",
    "            output1 = self.PCA1.transform(output1.reshape(1, -1))\n",
    "\n",
    "        if self.PCA2:\n",
    "            output2 = self.PCA2.transform(output2.reshape(1, -1))\n",
    "\n",
    "        return image, torch.FloatTensor(output1[0]), torch.FloatTensor(output2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset1(images_list= images, outputs_list1= brain_separate[0], outputs_list2 = brain_separate[1], transform=transforms.ToTensor(), PCA1 = PCA(n_components = 100), PCA2 = PCA(n_components = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
