{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add utils as a module and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "try:\n",
    "    from utils import data_loading\n",
    "except:\n",
    "    sys.path.append('../utils')\n",
    "    sys.path.append('../')\n",
    "    from utils import data_loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils import model\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 19004)\n",
      "(1000, 20544)\n",
      "(425, 425, 3)\n"
     ]
    }
   ],
   "source": [
    "#loading data for subject 1, first 10 images\n",
    "lh, rh, images  = data_loading.load_subject_data(1, 0, 1000)\n",
    "\n",
    "print(lh.shape)\n",
    "print(rh.shape)\n",
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat lh and rh\n",
    "brain = np.concatenate((lh, rh), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset for one-head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       \n",
      "Initialize CustomDataset \n",
      "--------\n",
      "Number of samples:  1000\n",
      "Transform:  ToTensor()\n",
      "PCA:  PCA(n_components=100)\n",
      "\n",
      "Data loaded\n",
      "-------\n",
      "Data:  1000 *  (<PIL.Image.Image image mode=RGB size=425x425 at 0x7FF510338DF0>, array([-0.8617882 , -0.20318632, -0.62639767, ..., -0.41889378,\n",
      "       -0.60231453, -0.67537224], dtype=float32))\n",
      "Output_concat:  1000 * 39548 :  [-0.8617882  -0.20318632 -0.62639767 ... -0.41889378 -0.60231453\n",
      " -0.67537224]\n",
      "\n",
      "Dataset made up of  1000 pairs of data\n",
      "--------\n",
      "Shape of 1st element in pair: torch.Size([3, 425, 425])\n",
      "Shape of 2nd element in pair: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "dataset = data_loading.CustomDataset(images_list= images, outputs_list = brain, transform=transforms.ToTensor(), PCA = PCA(n_components = 100))\n",
    "print('\\nDataset made up of ', len(dataset), 'pairs of data\\n--------')\n",
    "print('Shape of 1st element in pair:', dataset[0][0].shape)\n",
    "print('Shape of 2nd element in pair:', dataset[0][1].shape)\n",
    "\n",
    "#Create a train and validation subset of variable dataset with torch\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "#put train dataset into a loader with 2 batches and put test data in val loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for 2head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1000\n",
      "Output_concat1: 19004\n",
      "Output_concat2: 20544\n",
      "\n",
      "Dataset made up of  1000 truples of data\n",
      "--------\n",
      "Shape of 1st element: torch.Size([3, 425, 425])\n",
      "Shape of 2nd element: torch.Size([100])\n",
      "Shape of 3nd element: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "dataset2 = CustomDataset1(images_list= images, outputs_list1 = lh, outputs_list2 = rh, transform=transforms.ToTensor(), PCA1 = PCA(n_components = 100), PCA2 = PCA(n_components = 100))\n",
    "print('\\nDataset made up of ', len(dataset2), 'truples of data\\n--------')\n",
    "print('Shape of 1st element:', dataset2[0][0].shape)\n",
    "print('Shape of 2nd element:', dataset2[0][1].shape)\n",
    "print('Shape of 3nd element:', dataset2[0][2].shape)\n",
    "\n",
    "#Create a train and validation subset of variable dataset with torch\n",
    "train_size = int(0.8 * len(dataset2))\n",
    "test_size = len(dataset2) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset2, [train_size, test_size])\n",
    "#put train dataset into a loader with 2 batches and put test data in val loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = model.get_pretrained_regression_model(100)\n",
    "reg_model2 = model.ResNet2HeadModel(100)\n",
    "trainer = model.Trainer()\n",
    "optimizer = torch.optim.Adam\n",
    "loss = torch.nn.MSELoss()\n",
    "trainer.compile(reg_model2, optimizer, learning_rate=0.0001, loss_fn=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet2HeadModel(\n",
       "  (pretrained_model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Training Loss: 149.629443359375\n",
      "Validation Loss: 139.35010940551757\n",
      "Epoch [2/3], Training Loss: 147.460174407959\n",
      "Validation Loss: 139.1799237060547\n",
      "Epoch [3/3], Training Loss: 144.93706481933594\n",
      "Validation Loss: 139.5717193222046\n"
     ]
    }
   ],
   "source": [
    "trainer.fit_dual_head(num_epochs= 3, train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 425, 425])\n",
      "torch.Size([16, 100])\n",
      "tensor([[-6.9004e+01,  5.2036e+01, -1.3860e+01,  ..., -1.1964e+00,\n",
      "         -5.9054e+00, -3.0465e+00],\n",
      "        [ 2.8790e+01, -6.7930e+01,  5.3093e+01,  ...,  2.5009e+00,\n",
      "         -1.1363e+00, -9.1270e+00],\n",
      "        [-1.0563e+01,  3.1635e+01, -6.0819e+01,  ..., -2.8365e+00,\n",
      "         -1.0647e-02, -1.1718e+00],\n",
      "        ...,\n",
      "        [ 4.8215e+01,  1.5110e+01,  5.9402e+01,  ..., -1.2612e+01,\n",
      "         -4.5678e+00, -9.4691e+00],\n",
      "        [ 1.1411e+01, -2.3718e+01, -3.3905e+01,  ..., -1.1347e+01,\n",
      "          9.9543e+00, -1.3719e+00],\n",
      "        [-3.7915e+01,  1.9135e+01, -2.4873e+01,  ..., -4.2263e+00,\n",
      "         -4.8566e-01, -1.6125e+00]])\n"
     ]
    }
   ],
   "source": [
    "for inputs in train_loader:\n",
    "    print(inputs[0][0].shape)\n",
    "    print(inputs[1].shape)\n",
    "    # print(inputs[2].shape)\n",
    "    print(inputs[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and training history saved to initial_model.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.save('initial_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aleksygalkowski/Documents/Projects/ucph/social-data-science/2023 sem 3/aml-itu/aml_project_2023/src/notebooks/initial_model.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aleksygalkowski/Documents/Projects/ucph/social-data-science/2023%20sem%203/aml-itu/aml_project_2023/src/notebooks/initial_model.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "train_dataset[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer  = model.Trainer()\n",
    "trainer.compile(reg_model, optimizer, learning_rate=0.0001, loss_fn=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and training history loaded from initial_model.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.load('initial_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [170.70301513671876, 167.72671508789062],\n",
       " 'val_loss': [183.5020439147949, 183.56376399993897]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset1(Dataset):\n",
    "    def __init__(self, images_list, outputs_list1, outputs_list2, transform=None, PCA1=None, PCA2=None):\n",
    "        self.num_samples = len(images_list)\n",
    "        print('Number of samples:', self.num_samples)\n",
    "        self.transform = transform\n",
    "        self.PCA1 = PCA1\n",
    "        self.PCA2 = PCA2\n",
    "        self.data, self.output1, self.output2 = self.load_data(images_list, outputs_list1, outputs_list2)\n",
    "\n",
    "    def load_data(self, images_list, outputs_list1, outputs_list2):\n",
    "        data = []\n",
    "        output_concat1 = []\n",
    "        output_concat2 = []\n",
    "\n",
    "        for i in range(self.num_samples):\n",
    "            # Load image from the given list\n",
    "            image = Image.fromarray(images_list[i])\n",
    "\n",
    "            # Load output arrays from the given lists\n",
    "            output1 = outputs_list1[i]\n",
    "            output2 = outputs_list2[i]\n",
    "\n",
    "            data.append((image, output1, output2))\n",
    "            output_concat1.append(output1)\n",
    "            output_concat2.append(output2)\n",
    "\n",
    "        if self.PCA1:\n",
    "            self.PCA1.fit(output_concat1)\n",
    "\n",
    "        if self.PCA2:\n",
    "            self.PCA2.fit(output_concat2)\n",
    "\n",
    "        print('Output_concat1:', len(output_concat1[0]))\n",
    "        print('Output_concat2:', len(output_concat2[0]))\n",
    "        return data, output_concat1, output_concat2\n",
    "\n",
    "    def give_output(self):\n",
    "        return self.output1, self.output2\n",
    "\n",
    "    def get_PCA1(self):\n",
    "        return self.PCA1\n",
    "\n",
    "    def get_PCA2(self):\n",
    "        return self.PCA2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, output1, output2 = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.PCA1:\n",
    "            output1 = self.PCA1.transform(output1.reshape(1, -1))\n",
    "\n",
    "        if self.PCA2:\n",
    "            output2 = self.PCA2.transform(output2.reshape(1, -1))\n",
    "\n",
    "        return image, torch.FloatTensor(output1[0]), torch.FloatTensor(output2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset1(images_list= images, outputs_list1= brain_separate[0], outputs_list2 = brain_separate[1], transform=transforms.ToTensor(), PCA1 = PCA(n_components = 100), PCA2 = PCA(n_components = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
