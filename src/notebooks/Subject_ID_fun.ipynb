{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from utils import data_loading, model\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path, start=None, end=None):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path)[start:end]:\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        # .transpose((2, 0, 1))\n",
    "        images.append(img_array)\n",
    "    return images\n",
    "\n",
    "def load_subject_data(subject, index_start=None, index_end=None, return_dict=False):\n",
    "    current_dir = os.getcwd()\n",
    "    path = '../../data/algonauts/subj0' + str(subject)\n",
    "    data_lh = np.load(path + '/training_split/training_fmri/lh_training_fmri.npy')[index_start : index_end]\n",
    "    data_rh = np.load(path + '/training_split/training_fmri/rh_training_fmri.npy')[index_start : index_end]\n",
    "    folder_path = path+\"/training_split/training_images/\"\n",
    "    image_data = load_images_from_folder(folder_path, index_start, index_end)\n",
    "    id_list = [subject for subject in range(len(image_data))]\n",
    "    return data_lh, data_rh, image_data, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1_lh, sub1_rh, image1, id1 = load_subject_data(1, 0, 10)\n",
    "sub2_lh, sub2_rh, image2, id2 = load_subject_data(2, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain1 = np.concatenate((sub1_lh, sub1_rh), axis=1)\n",
    "brain2 = np.concatenate((sub2_lh, sub2_rh), axis=1)\n",
    "brain = np.concatenate((brain1, brain2), axis=0)\n",
    "ids = id1 + id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = image1 + image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       \n",
      "Initialize CustomDataset \n",
      "--------\n",
      "Number of samples:  20\n",
      "Transform:  ToTensor()\n",
      "PCA:  None\n",
      "\n",
      "Data loaded\n",
      "-------\n",
      "Data:  20 *  ([<PIL.Image.Image image mode=RGB size=425x425 at 0x198B39BB0A0>, 0], array([-0.8617882 , -0.20318632, -0.62639767, ..., -0.41889378,\n",
      "       -0.60231453, -0.67537224], dtype=float32))\n",
      "Output_concat:  20 * 39548 :  [-0.8617882  -0.20318632 -0.62639767 ... -0.41889378 -0.60231453\n",
      " -0.67537224]\n"
     ]
    }
   ],
   "source": [
    "dataset = data_loading.CustomDataset(images_list=images, outputs_list=brain, transform=transforms.ToTensor(), id_list=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset made up of  20 pairs of data\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Function: fit:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got numpy.float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\src\\notebooks\\Subject_ID_fun.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/frbre/OneDrive/01%20Dokumenter/01%20Uni/01%20Advanced%20Machine%20Learning/aml_project_2023/src/notebooks/Subject_ID_fun.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/frbre/OneDrive/01%20Dokumenter/01%20Uni/01%20Advanced%20Machine%20Learning/aml_project_2023/src/notebooks/Subject_ID_fun.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m trainer\u001b[39m.\u001b[39mcompile(reg_model, optimizer, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, loss_fn\u001b[39m=\u001b[39mloss)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/frbre/OneDrive/01%20Dokumenter/01%20Uni/01%20Advanced%20Machine%20Learning/aml_project_2023/src/notebooks/Subject_ID_fun.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(num_epochs \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader)\n",
      "File \u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\src\\notebooks\\..\\utils\\model.py:142\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, num_epochs, train_loader, val_loader)\u001b[0m\n\u001b[0;32m    140\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader), desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Function: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader:\n\u001b[0;32m    143\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    144\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\n",
      "File \u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\aml_project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\aml_project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\aml_project\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\aml_project\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\aml_project\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\frbre\\OneDrive\\01 Dokumenter\\01 Uni\\01 Advanced Machine Learning\\aml_project_2023\\src\\notebooks\\..\\utils\\data_loading.py:101\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     98\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPCA\u001b[39m.\u001b[39mtransform(output\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_list:\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m (image, subject_id), torch\u001b[39m.\u001b[39;49mFloatTensor(output[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m image, torch\u001b[39m.\u001b[39mFloatTensor(output[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): data must be a sequence (got numpy.float32)"
     ]
    }
   ],
   "source": [
    "print('\\nDataset made up of ', len(dataset), 'pairs of data\\n--------')\n",
    "print('Shape of 1st element in pair:', dataset[0][0].shape)\n",
    "print('Shape of 2nd element in pair:', dataset[0][1].shape)\n",
    "\n",
    "# Create a train and validation subset of variable dataset with torch\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Put train dataset into a loader with 2 batches and put test data in val loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)\n",
    "\n",
    "# Initialize model, trainer, optimizer and loss function\n",
    "reg_model = model.get_pretrained_regression_model(100)\n",
    "trainer = model.Trainer()\n",
    "optimizer = torch.optim.Adam\n",
    "loss = torch.nn.MSELoss()\n",
    "trainer.compile(reg_model, optimizer, learning_rate=0.0001, loss_fn=loss)\n",
    "\n",
    "trainer.fit(num_epochs = 3, train_loader=train_loader, val_loader=val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
